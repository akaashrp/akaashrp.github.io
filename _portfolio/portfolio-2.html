---
title: "Vasuki Research Project"
excerpt: ""
collection: portfolio
---

This is the accompanying report for a group research project I worked on called "Vasuki: Minimizing Makespan for Offline LLM Batch Inference". The project was focused on developing a first-class offline LLM inference serving system leveraging KV cache offloading and bin-packing to navigate the induced memory-makespan-cost tradeoff space on commodity hardware. The project report is available for download <a href="/files/vasuki.pdf" target="_blank">here</a>.
<!-- This is an item in your portfolio. It can be have images or nice text. If you name the file .md, it will be parsed as markdown. If you name the file .html, it will be parsed as HTML.  -->
<!-- <embed src="/files/vasuki.pdf" width="500" height="375"> -->
